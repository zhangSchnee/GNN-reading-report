   按照第一周的计划上周我主要看了GCN的两篇paper中的理论和数学推导，理解了GCN的数学原理：GCN是CNN(卷积神经网络)在图这种非欧几里得空间数据结构上的推广。根据卷积的定义(两个函数的卷积等于两个函数频域乘积的逆变换)，人们根据拉普拉斯矩阵的性质构造出了傅里叶(逆)变换在graph上的形式，从而得到了图上的卷积神经网络的形式。
   
   实际上讲，一个信号的时域傅里叶变换的离散化表达就是信号向量和e^jwt向量的乘积加和，天然地满足矩阵形式。而e^jwt是广义的laplacian矩阵的特征向量，因此e^jwt映射到图上就是U(将laplaican做分解)。但是我们都知道算U很复杂，而几个主特征值已经可以较好的刻画U了，因此之后人们做的工作就是：能不能找到好的卷积核，从而使得计算U的复杂度下降(甚至不用算)等等；经过选用恰当的卷积核人们发现了一个虽然不是最优，但数学上看起来很舒服的一个形式：![](https://latex.codecogs.com/gif.latex?y&space;=&space;\sum_{j=0}^{K}\alpha_{j}L^jx)
   
   而巧妙的地方在于L\*x可以和热传导方程连上关系，图上节点信息量的变化不也是受到邻居节点、邻居的邻居...节点的影响嘛。
   
   和CNN一样，GCN也有局部节点的信息提取和参数共享，人们通过改变卷积核的大小来调节卷积核在图上感受野的范围。目前了解到的GCN理论研究重点是设计性质更加好的卷积核，使得计算量更小，更有效提取节点和边信息等。这周的计划是阅读一篇GCN应用到关系抽取的文章，看一看实际应用是怎样的。

---
2020/3/9

#### Graph Convolution over Pruned Dependency Trees Improves Relation Extraction

这周按照计划读了EMNLP 2018的一篇文章，感觉这篇文章的理论方法和写作的构思立意都值得我去思考。
-   从知识的角度想这篇文章把减枝后的句法依存树当作图来做GCN，同时根据实践在kipf提出的GCN形式上输入时加入了一层双向LSTM层(C-GCN)来使得输入具有序列性，这样既可以对图做GCN又能在图的节点处保留来原来句子的序列性信息。从实验上说明了原来的方法(Tree-LSTM)更适合做局部特征关系的提取，而GCN方法对于远距离实体关系抽取有着更好的表现。另外根据关系抽取任务设置了task-specific的减枝策略。 
-   这篇文章的构思挺好，模型并不复杂却有创新，给看的人来说它解决了如何把GCN理论套用到关系抽取这一领域的问题，另外通过充足的实验和图示说明了GCN这种方法和旧的方法比优点在哪；在文章中让人看到了新的理论以及好的实验结果，又能解决原来存在的问题，个人感觉这种写法还是挺适合我学习的。

---
2020/3/16
#### Context-Aware Representations for Knowledge Base Relation Extraction

文章很短，应该是short paper。感觉亮点有两个，首先是他们考虑到一个句子中除了要抽取目标实体间关系之外(下面简称为目标关系)有多个其它关系的情况，这些其它的关系有时候会帮助主要的关系抽取，有时候则会阻碍，因此处理这种跟具体文本有关的模型叫做Context-Aware。这种考虑和实际情况还是挺接近的。

这篇文章是利用多个BI-LSTM分别抽取除了目标关系之外的不同的关系，然后用一个attention layer去判别和学习这些额外的关系对目标关系的影响，如果促进就增加权重当作额外关系，如果是抑制就减小权重相当于过滤噪声。

第二个亮点在于他自己用 wikipedia构建了关系抽取的一个数据集，方法我觉得还是值得借鉴的，拓宽了我的思路，见下图。
![avatar](./img/Wikipedia_RE_Process.png)

另外还看了一篇关于GNN做link prediction和 Entity classification，但是看完了不是很明白，感觉还要多读一下。里面关于GCN是消息传播的一种形式的讨论以及选取合适的参数共享方式防止过拟合的方法我认为还是很值得学习的。
先做个flag以后补
#### Modeling Relational Data with Graph Convolutional Networks
咕咕

---
2020/3/23
#### Graph Neural Networks with Generated Parameters for Relation Extraction

感觉很棒！这篇文章和Context-Aware RE的那篇文章出发点类似，都是考虑当一个句子中存在多个实体以及实体间关系(multi-hop relation)的情况。但是Context-Aware LSTM判别multi-hop relation的方法是基于attention mechanism，因此其它所有实体都会对当前判别的实体间关系产生影响，难免会产生噪声。而GCN每一次迭代只受相邻节点的影响则能缓解这一点(这个结论是作者对实验结果的一些解释，不过作者用了may的意思应该也是不太确定)。该文章的另一个创新点在于提出了可变参数的边/邻接矩阵，可以不用预先定义句子中图的结构直接在非结构化句子中进行多实体关系的抽取。最后该文章用实验说明了GP-GNN在全监督/远监督数据集上的优越效果以及在多跳推理机制上的优点。

GP-GNN网络的设计也具有范式意义，个人认为GCN的设计关键在于三点：怎么设计合适的图卷积核，怎么合理的根据任务初始化节点以及得到节点表示之后怎么利用他们做分类；在这篇文章中它把整个网络分为编码、迭代以及判别模块。每一个模块都解决了上述的一个问题，编码模块通过双向lSTM和MLP根据两个实体间的词生成可变参数的边，其中双向LSTM的输入是word+pos embedding，迭代模块利用编码模块生成的邻接矩阵做graph convolution,人工将关系三元组中的subject和object加入节点中做节点初始化，最后判别模块使用迭代后生成的图节点特征进行关系的实体关系的判别。

但是这篇文章还有些地方没有明白，准备问一下nlp的老师，首先文中提到了编码部分的参数每一层都不一样，意思是每一层用不同的MLE(LSTM\[\.\])生成表示吗？另外是关于图的构建，个人推测是根据标注的entity来构建，并不是一个complete graph，因为没有代码不知道具体实现是怎样的，还需要问一下。

- 2020/4/23 update: 现在了解到这个模型参数每一层可以不一样，也可以是pinned,图的构建是一个complete graph所以计算量会很大；另外个人感觉GCN和GAT最大的不同还是在于GCN的边并不依赖于节点，和节点一样是图的另一种结构；但是GAT就是边是用节点间的attention学出来的

---
2020/3/30
#### GRAPH ATTENTION NETWORKS

这周看了yoshua Bengio提出的GRAPH ATTENTION NETWORKS(GAT)，GAT是GNN的另一种分支，和GCN不同的地方在于，GAT直接在图上(而不是在图的谱上)定义卷积，因此设计的关键就在于如何定义一个可以处理可变数目邻居节点并且能够共享权重的卷积核。GAT的实现思路是图中的节点受邻居节点的影响，并且这种影响的权重是不一样的，因此可以通过attention机制来学出这种不同的权重；值得学习的是，该论文还考虑了有向图中两个相互连结的节点彼此之间的影响是不一样的这种特性，因此通过把self attention也concat进来的方式来实现了一种非对称的attention。另外论文中采用了multi-head attention，论文中讲到这种思想是出自CNN中不同尺度的卷积核，个人感觉这是一种让模型更加鲁棒的做法。

从本质上来说，GCN和GAT都属于message passing，都是将邻居节点的特征聚合到中心定点上。但是GAT和GCN还是有着许多的不同，设计上GAT通过图节点间attention的不用权重体现出了边有方向的特性，因此GAT可以处理有向图(GCN不可以直接在有向图上应用，因为谱分解需要邻接矩阵的对称性)。因为GAT是直接在节点和相邻节点地子图上进行操作，整个计算是局部的，不要事先知道整个图的结构，这一点使得GAT可以处理动态图。最后和kipf提出的GCN相比，GAT可以给同阶里面不同的邻居分配不同的权重，这一点是GCN难以做到的。


看了这么多篇文章，之后准备开始看看代码和想想实现方式了。最近写word2vec感觉真的不能纸上谈兵。