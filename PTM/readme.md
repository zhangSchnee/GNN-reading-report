2020/4/13
#### ERNIE2.0
上周看了一下百度提出的一个中文预训练模型的框架ERNIE，看了之后感觉有两点值得我学习的东西，第一个是该框架将知识库中的知识融合到了模型的预训练当中，具体的操作是在预训练阶段相比于bert的随机mask不同的字，根据知识mask不同的实体，这样能够更好的学到中文句子的文本信息；这种在预训练模型中引入知识库的做法还是很值得学习的，如何把结构化的先验知识融合到模型里从而使得模型有更好的理解能力感觉可能是未来研究的一个好的方向。
第二个是模型的预训练采用了连续多任务学习的方法，将连续学习和多任务学习融合在一起来训练网络；训练的过程个人感觉是两层的for循环，大循环中每次加入一个新的训练任务，小循环中对每个训练任务训练N个epoch，不过这种手段感觉更像是工业界的应用。